{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load in the data in `companies.csv` and `prices.csv` (in the data folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function `is_incorporated` that checks whether an input string, `name`, contains the substring \"inc\" or \"Inc\". Its definition should look like this:\n",
    "```python\n",
    "def is_incorporated(name):\n",
    "```\n",
    "(Yes, all these companies are *technically* incorporated, but bear with us for the exercise.)\n",
    "<br>\n",
    "*Hint: you may want to google something like \"check if one string is within another Python\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test this function to be sure it works. Try passing in some strings that contain the substring and some that don't. Test it on data from the companies DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a `for` loop to iterate through the elements in the Name column of the companies data, applying `is_incorporated` to each element and printing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now rewrite the code for #4 using the `Series.apply` method -- apply the function to the Series and print the resulting Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. *Similar, but less guided.* Create a new column, name_length, whose value is:\n",
    "    - `\"long\"` if the company name is over 12 characters\n",
    "    - `\"medium\"` if the company name is 8-11 characters\n",
    "    - `\"short\"` if the company name is 7 or fewer characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a function `make_colname_string` that takes a DataFrame as an argument and returns a string that contains all the DataFrame's columns' names, comma separated. For example, running `make_colname_string` on our companies data would look like this:<br><br>\n",
    "```python\n",
    "make_colname_string(companies)\n",
    "#> 'Symbol,Name,Sector'\n",
    "```\n",
    "<br>*Hint: a `for` loop will be helpful.*\n",
    "<br>Test it on the prices data. What does it return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "Suppose you've discovered a great secret about the stock market: companies with long names (as defined above) are going to double in value after quarter 4 (our most recent data), companies with short names are going to halve in value, and medium-name companies will stay exactly the same. Create a dataset of the form:\n",
    "\n",
    "| Name | Symbol | Projected |\n",
    "-------|--------|------------\n",
    "\n",
    "Where \"Projected\" is the projected price of the company's stock (2x, 1x, .5x as explained above). Note that you will need to join companies to prices and do some data wrangling operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Modeling*\n",
    "\n",
    "The Boston Housing data set is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. Originally published in [Harrison & Rubinfeld (1978)](https://deepblue.lib.umich.edu/handle/2027.42/22636), it contains 13 attributes to predict the median property value.\n",
    "\n",
    "- __problem type__: supervised regression\n",
    "- __response variable__: `medv` median value of owner-occupied homes in USD 1000's (i.e. 21.8, 24.5)\n",
    "- __features__: 13 \n",
    "- __observations__: 506\n",
    "- __objective__: use property attributes to predict the median value of owner-occupied homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load in the data in `boston.csv` (in the data folder). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore the data:\n",
    "  * How many rows are in the data?\n",
    "  * How many columns are in the data?\n",
    "  * What data type is each column?\n",
    "  * Can you guess what the features represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the data into a training set and test set using a 70-30% split.\n",
    "\n",
    "   - How many observations are in the training set and test set?\n",
    "   - Compare the distribution of `cmedv` between the training set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Separate the features from the label (`cmedv`). You can go ahead and use all the features since they are all numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fit a default KNN regression model (`KNeighborsRegressor()`). For this step do not use _k_-fold validation. What is the default MSE/RMSE for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Perform a 10-fold cross-validation for the default KNN model. \n",
    "\n",
    "   - Use the `neg_root_mean_squared_error` loss function.\n",
    "   - Use `shuffle=True`.\n",
    "   - Describe the results (i.e. what is the average RMSE across all 10 folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Now perform a hyperparameter grid search where _k_ ranges from 2--20 and apply 10-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. __STRETCH EXERCISE__: Perform a new model pipeline using:\n",
    "\n",
    "* The `RandomForestRegressor` learning (available via `sklearn.ensemble` module)\n",
    "* Perform a grid search for two hyperparameters (this grid search may take ~2 min):\n",
    "   - `'n_estimators': [200, 400, 800]`\n",
    "   - `'max_features': [4, 8, 12]`\n",
    "* Use the same `kfold` and `loss` objects defined in exercise #7\n",
    "* What is the best model's cross validated RMSE?\n",
    "* How does this compare to the KNN model?\n",
    "* What are the best model's hyperparameter settings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Python Environments*\n",
    "\n",
    "7. While training and running your model should have been pretty fast, imagine that you are working with orders of magnitude more data -- so you want to train the model overnight, rather than interactively.\n",
    "  * Export your Jupyter notebook as a `.py` script.\n",
    "  * Be sure to add `print()` calls so you can see important data.\n",
    "  * Run your `.py` file from the command line, and verify the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Think back to Lesson 8, on the data science ecosystem. \n",
    "  * Which package sounded most interesting/useful to you? \n",
    "  * If you are working on a platform with `conda`, create a new environment called \"temporary\".\n",
    "  * Activate the `temporary` environment.\n",
    "  * Install your package of choice package in `temporary`. \n",
    "  * Try to import it in a notebook (remember you'll need to get your conda environment working in Jupyter), and look in the online documentation (just google \"package-name docs\") to figure out what you can do with this package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
